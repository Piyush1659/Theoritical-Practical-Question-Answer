{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoritical Questions"
      ],
      "metadata": {
        "id": "8GHT9wQgpZA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What does R-squared represent in a regression model?\n",
        "\n",
        ">- R-squared, also known as the coefficient of determination, is a statistical measure in regression analysis that represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in the model. In simpler terms, it indicates how well the model's predictions align with the actual data, with higher R-squared values suggesting a better fit."
      ],
      "metadata": {
        "id": "kWIsiAuGpbY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the assumptions of linear regression?\n",
        "\n",
        ">- Linear regression, a common statistical method, relies on several key assumptions to ensure the validity of its results. These assumptions include linearity, independence, homoscedasticity, and normality of residuals. Violations of these assumptions can lead to inaccurate predictions and unreliable conclusions."
      ],
      "metadata": {
        "id": "B5wPGxtxppcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        ">- R-squared and adjusted R-squared both measure how well a regression model fits the data, but adjusted R-squared penalizes the addition of irrelevant independent variables, making it a better indicator of model fit when comparing models with different numbers of predictors. R-squared always increases when a new predictor is added, even if it's not significant, while adjusted R-squared can decrease if the new predictor doesn't improve the model's explanatory power."
      ],
      "metadata": {
        "id": "9dCPBsDlpxry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        ">- Mean Squared Error (MSE) is primarily used to measure the average squared difference between predicted values and actual values in a statistical model. It quantifies the amount of error in a model's predictions, with lower MSE values indicating better model accuracy. Essentially, it helps assess how well a model's predictions align with the observed data."
      ],
      "metadata": {
        "id": "RJ6q4_SGp8k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What does an Adjusted R-squared value of 0.85 indicate.\n",
        "\n",
        ">- An adjusted R-squared value of 0.85 indicates that the model explains 85% of the variance in the dependent variable, while also accounting for the number of independent variables in the model. In simpler terms, it suggests a good fit for the model, meaning the independent variables chosen are effectively explaining the changes in the dependent variable, and the model isn't overly complex with unnecessary predictors."
      ],
      "metadata": {
        "id": "Lw2TU6bpqEvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we check for normality of residuals in linear regression?\n",
        "\n",
        ">- To check for the normality of residuals in linear regression, you can use both graphical and statistical methods. Visually, you can create a histogram or a Q-Q plot of the residuals. Statistically, you can perform a normality test like the Shapiro-Wilk test. A Q-Q plot compares the quantiles of your residuals against the quantiles of a normal distribution; a straight line suggests normality. Statistical tests like Shapiro-Wilk test the null hypothesis that the residuals are normally distributed."
      ],
      "metadata": {
        "id": "MmSWrJQ0qUzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is multicollinearity, and how does it impact regression?\n",
        "\n",
        ">- Multicollinearity, in essence, means that some of your predictor variables are highly related or even redundant. It arises when two or more independent variables in a multiple regression model are strongly correlated. This means that changes in one variable are associated with predictable changes in another.\n",
        "Impact on Regression Analysis:\n",
        "\n",
        "\n",
        "     1. Unreliable Coefficient Estimates:\n",
        "When multicollinearity exists, the estimated regression coefficients become very sensitive to small changes in the data or model specification. It becomes difficult to isolate the individual effect of each predictor variable on the dependent variable.  \n",
        "\n",
        "    2. Inflated Standard Errors:\n",
        "The standard errors of the regression coefficients tend to be larger in the presence of multicollinearity. This means that the confidence intervals for the coefficients are wider, making it harder to determine statistical significance.\n",
        "\n",
        "    3. Instability of the Model:\n",
        "The model becomes less stable, meaning that small changes in the data or model specification can lead to significant changes in the estimated coefficients and their significance.\n",
        "\n",
        "    4. Difficulties in Interpretation:\n",
        "It becomes challenging to interpret the individual effects of the predictor variables because they are not independent of each other.  \n",
        "\n",
        "    5. Potential for Overfitting:\n",
        "Multicollinearity can lead to overfitting, where the model fits the training data very well but does not generalize well to new, unseen data."
      ],
      "metadata": {
        "id": "lDER3pZPqg44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Mean Absolute Error (MAE)?\n",
        "\n",
        ">- Mean Absolute Error (MAE) is a metric used in regression analysis to evaluate the accuracy of a model's predictions. It measures the average magnitude of errors between predicted and actual values by calculating the average of the absolute differences between them. According to online sources."
      ],
      "metadata": {
        "id": "Qg7ZgySEq6TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the benefits of using an ML pipeline?\n",
        "\n",
        ">- ML pipelines offer numerous benefits, including increased efficiency, reproducibility, and collaboration, by automating and standardizing the machine learning workflow. They enable faster model development, easier deployment, and better monitoring, leading to more robust and reliable solutions."
      ],
      "metadata": {
        "id": "TKg3VD3KrBNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Why is RMSE considered more interpretable than MSE?\n",
        "\n",
        ">- nterpretability on Scale: RMSE, unlike MSE, is on the same scale as the original data, making it more interpretable. Sensitive to Large Errors: Like MSE, RMSE also penalizes large errors, but since it's on the original scale, it can provide a more intuitive measure of the error magnitude"
      ],
      "metadata": {
        "id": "-7Zx0FnSrITC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is pickling in Python, and how is it useful in ML?\n",
        "\n",
        ">- In Python, pickling is the process of serializing a Python object structure into a byte stream, which can then be stored in a file or transmitted over a network. The reverse process, deserialization, is called unpickling, which converts the byte stream back into a Python object. This is useful in machine learning (ML) for saving trained models and their states, allowing for later reuse without retraining.\n",
        "\n",
        "How Pickling Works:\n",
        "\n",
        "     1. Serialization:\n",
        "The pickle.dump() function takes a Python object and converts it into a byte stream, which is then written to a file or transmitted.\n",
        "\n",
        "     2. Deserialization:\n",
        "The pickle.load() function takes the byte stream from a file or other source and reconstructs the original Python object.\n",
        "\n",
        "\n",
        "Benefits in ML:\n",
        "\n",
        "    Saving Trained Models:\n",
        "Pickling allows you to save a trained ML model (including its parameters and internal state) to a file after training.\n",
        "\n",
        "    Reusing Models:\n",
        "You can load the pickled model later and use it for predictions or further training without having to retrain the entire model from scratch.\n",
        "\n",
        "    Sharing Models:\n",
        "Pickled models can be easily shared with others, allowing them to utilize your trained model without needing to reproduce the training process.\n",
        "\n",
        "    Inter-process Communication:\n",
        "Pickling can be used to serialize and transmit models between different processes or even different machines, facilitating distributed ML workflows"
      ],
      "metadata": {
        "id": "UBQuZEovrTA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What does a high R-squared value mean?\n",
        "\n",
        ">- A high R-squared value, which ranges from 0 to 1 (or 0% to 100%), indicates that a larger proportion of the variance in the dependent variable is explained by the independent variables in a regression model. In simpler terms, it suggests a better fit of the model to the data, meaning the model's predictions are more closely aligned with the actual observed values."
      ],
      "metadata": {
        "id": "Jzydf0pXrrI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What happens if linear regression assumptions are violated?\n",
        "\n",
        ">- Violating linear regression assumptions can lead to unreliable and misleading results. Specifically, it can result in biased coefficient estimates, inaccurate confidence intervals, and unreliable hypothesis tests. The model might not accurately represent the true relationship between variables and may lead to incorrect predictions and interpretations."
      ],
      "metadata": {
        "id": "mU6PSZHarypX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can we address multicollinearity in regression?\n",
        "\n",
        ">- Multicollinearity, the presence of high correlation among independent variables in a regression model, can be addressed through several techniques. These include removing highly correlated predictors, combining them, using regularization methods, increasing sample size, or employing variable transformation techniques."
      ],
      "metadata": {
        "id": "ATK4CpMFr5pI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can feature selection improve model performance in regression analysis?\n",
        "\n",
        ">- Feature selection significantly enhances regression model performance by improving accuracy, reducing overfitting, and speeding up training. By identifying and using only the most relevant features, it minimizes noise and redundancy in the data, leading to a more focused and effective model."
      ],
      "metadata": {
        "id": "KCjMNtdcsBBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  How is Adjusted R-squared calculated?\n",
        "\n",
        ">- The formula for adjusted R-squared is: Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)] where R² is the regular R-squared, n is the number of data points, and k is the number of independent variables in the model.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "R² (R-squared): Represents the proportion of variance in the dependent variable that is explained by the independent variables in the regression model. It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "\n",
        "n (Number of data points): The total number of observations in your dataset.\n",
        "k (Number of independent variables): The number of predictor variables included in the regression model."
      ],
      "metadata": {
        "id": "b3LomHUVsJGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Why is MSE sensitive to outliers?\n",
        "\n",
        ">- MSE (Mean Squared Error) is sensitive to outliers because it squares the differences between predicted and actual values. Squaring these differences amplifies the impact of larger errors, making the MSE more susceptible to outliers than metrics like Mean Absolute Error (MAE) which do not square the errors."
      ],
      "metadata": {
        "id": "1pZPuFVusWyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What is the role of homoscedasticity in linear regression?\n",
        "\n",
        ">- In linear regression, homoscedasticity, meaning \"same variance,\" is the assumption that the variability of the error terms (residuals) is constant across all levels of the independent variables. Essentially, the spread of the data points around the regression line should be consistent, regardless of the values of the predictor variables. This assumption is crucial for the validity and reliability of statistical inferences drawn from the regression model."
      ],
      "metadata": {
        "id": "1TpJ3h3tseG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is Root Mean Squared Error (RMSE)?\n",
        "\n",
        ">- Root Mean Squared Error (RMSE) is a commonly used metric in machine learning to evaluate the accuracy of predictive models. It quantifies the average magnitude of the errors between predicted and actual values by calculating the square root of the average of squared differences."
      ],
      "metadata": {
        "id": "jovM-Z3Vsn3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Why is pickling considered risky?\n",
        "\n",
        ">- Pickling can be risky due to potential bacterial contamination, especially in homemade pickles, which can lead to foodborne illnesses like botulism. Additionally, high salt content in pickles can pose health risks for individuals with hypertension or kidney problems. Some studies have also linked pickled vegetable consumption to increased risks of certain cancers."
      ],
      "metadata": {
        "id": "rjhrbCeKsvSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What alternatives exist to pickling for saving ML models?\n",
        "\n",
        ">- The SciPy library in Python offers an easy alternative to pickle files for storing a machine learning model, Joblib. It is best suited for cases that involve storing large NumPy arrays. For Python 3.8, Joblib is useful in storing objects with nested NumPy arrays in memory maped mode."
      ],
      "metadata": {
        "id": "o7aqV2cXs2EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is heteroscedasticity, and why is it a problem?\n",
        "\n",
        ">- Heteroscedasticity, in regression analysis, is the condition where the variability (variance) of a variable is unequal across the range of values of another variable that predicts it. In simpler terms, it means the \"spread\" or \"scatter\" of the data points is not consistent. This violates a key assumption of ordinary least squares (OLS) regression, which assumes homoscedasticity (constant variance).\n",
        "\n",
        "Why is heteroscedasticity a problem?\n",
        "\n",
        "    1. Biased Standard Errors:\n",
        "Heteroscedasticity can lead to biased and inefficient estimates of the standard errors of regression coefficients. This means the confidence intervals and hypothesis tests based on these standard errors become unreliable.\n",
        "\n",
        "    2. Incorrect Significance Tests:\n",
        "Because the standard errors are incorrect, the t-statistics and p-values used for hypothesis testing are also unreliable. You might falsely conclude that a variable is statistically significant when it is not, or vice versa.\n",
        "\n",
        "    3. Inefficient Estimates:\n",
        "OLS estimators are no longer the most efficient (i.e., have the smallest variance) when heteroscedasticity is present.\n",
        "\n",
        "    4. Poor Predictions:\n",
        "Heteroscedasticity can lead to less accurate predictions, especially in regions of the data where the variance is higher.\n",
        "\n",
        "    5. Misleading Inference:\n",
        "Overall, heteroscedasticity can lead to incorrect conclusions about the relationships between variables in your model."
      ],
      "metadata": {
        "id": "6iCfBa35s-cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  How can interaction terms enhance a regression model's predictive power?\n",
        "\n",
        ">- Interaction terms in regression models can significantly enhance predictive power by capturing how the relationship between variables changes based on the levels of other variables. This allows the model to better represent complex, non-linear relationships, leading to more accurate predictions."
      ],
      "metadata": {
        "id": "DHUqFtpdtPPx"
      }
    }
  ]
}