{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Regression _Assignment"
      ],
      "metadata": {
        "id": "q-CyAZkp4ubg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression\n",
        "\n",
        ">- Simple Linear Regression is a statistical method used to model the relationship between two continuous variables by fitting a straight line to the observed data.\n",
        "\n",
        "    Definition:\n",
        "\n",
        ">- Simple Linear Regression describes how one independent variable (X) predicts the value of a dependent variable (Y) using a linear relationship."
      ],
      "metadata": {
        "id": "jfvPMNT44qoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        ">- The key assumptions of Simple Linear Regression (SLR) are as follows:\n",
        "\n",
        "    1. Linearity\n",
        "\n",
        "    There is a linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "    This means the expected change in Y is proportional to changes in X.\n",
        ".\n",
        "\n",
        "\n",
        "    2. Independence\n",
        "\n",
        "    The residuals (errors) are independent of each other.\n",
        "\n",
        "    In time series data, this implies no autocorrelation.\n",
        "\n",
        ".\n",
        "\n",
        "    3. Homoscedasticity (Constant Variance)\n",
        "\n",
        "    The variance of the residuals is constant across all levels of the independent variable.\n",
        "\n",
        "    In other words, the spread of errors should be similar for all predicted values.\n",
        ".\n",
        "\n",
        "\n",
        "    4. Normality of Residuals\n",
        "\n",
        "    The residuals (errors) are normally distributed, especially important for hypothesis testing and constructing confidence intervals.\n",
        "\n",
        "    This assumption is less critical for prediction but vital for inference.\n",
        "\n",
        ".\n",
        "\n",
        "    5. No or Minimal Multicollinearity (Primarily for Multiple Regression)\n",
        "\n",
        "    In simple linear regression with only one predictor, multicollinearity isn't applicable.\n",
        "\n",
        "    But the independent variable should not be a constant or near-constant.\n",
        "\n",
        ".\n",
        "\n",
        "    6. No Measurement Error in the Independent Variable\n",
        "\n",
        "    The X variable is assumed to be measured without error.\n",
        "\n",
        "    In practice, measurement error in X can bias the slope estimate."
      ],
      "metadata": {
        "id": "eZatvvaB5D6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        ">- In the equation y = mx + c, the coefficient m represents the slope or gradient of the straight line. The slope indicates how steep the line is and its direction (increasing or decreasing)."
      ],
      "metadata": {
        "id": "4R06RZUn5sDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        ">- In the equation Y = mX + c, the letter c represents the y-intercept. The y-intercept is the point where the line crosses the vertical (y) axis on a graph. It's the value of Y when X is equal to zero."
      ],
      "metadata": {
        "id": "kr0OEk8v5_To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        ">- In simple linear regression, the slope 'm' (also sometimes represented as 'b' in the equation y = mx + b) represents the rate of change of the dependent variable (y) for every one-unit change in the independent variable (x). It can be calculated using the formula: m = r * (Sy / Sx), where 'r' is the correlation coefficient between x and y, 'Sy' is the standard deviation of y, and 'Sx' is the standard deviation of x.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "Correlation Coefficient (r):\n",
        "This measures the strength and direction of the linear relationship between x and y, ranging from -1 to +1.\n",
        "\n",
        "Standard Deviation (Sy and Sx):\n",
        "\n",
        "These quantify the spread or variability of the y and x values, respectively.\n",
        "Formula in detail:\n",
        "\n",
        "The formula for the slope (m) can also be expressed as:\n",
        "\n",
        "m = [n * Σ(xy) - (Σx * Σy)] / [n * Σ(x²) - (Σx)²]\n",
        "\n",
        "Where:\n",
        "\n",
        "n is the number of data points.\n",
        "\n",
        "Σ(xy) is the sum of the product of each x and y value.\n",
        "\n",
        "Σx is the sum of all x values.\n",
        "\n",
        "Σy is the sum of all y values.\n",
        "\n",
        "Σ(x²) is the sum of the squares of each x value.\n",
        "\n",
        "This formula essentially calculates how much y changes for every unit change in x, taking into account the overall relationship between the two variables and their distributions."
      ],
      "metadata": {
        "id": "tTsrtmkH6Uwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        ">- The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line (regression line) through a set of data points by minimizing the sum of the squared differences between the observed values and the values predicted by the line."
      ],
      "metadata": {
        "id": "EotoLv7l63wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        ">- In Simple Linear Regression, the coefficient of determination (R²) is a statistical measure that explains how well the independent variable (predictor) explains the variability in the dependent variable (response).\n",
        "\n",
        "Interpretation of R²:\n",
        "\n",
        "    Definition:\n",
        "\n",
        "    R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "    R² ranges between 0 and 1:\n",
        "\n",
        "    R² = 1: Perfect fit; all the variability in the dependent variable is explained by the independent variable.\n",
        "\n",
        "    R² = 0: The model explains none of the variability; predictions are no better than using the mean of the dependent variable.\n",
        "\n",
        "    0 < R² < 1: Indicates the proportion of variance explained by the model."
      ],
      "metadata": {
        "id": "E44ziogj7EXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        ">- Multiple linear regression is a statistical method used to model the relationship between one dependent variable and two or more independent variables. It aims to find the best-fitting linear equation that describes how the independent variables collectively influence the dependent variable. This technique is widely used for prediction, pattern detection, and understanding the impact of multiple factors on a single outcome.\n"
      ],
      "metadata": {
        "id": "QaXoK7aG7sQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        ">- The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. Simple linear regression uses only one independent variable, while multiple linear regression uses two or more.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "    1. Simple Linear Regression:\n",
        "\n",
        "    This type of regression examines the relationship between one independent (predictor) variable and a single dependent (outcome) variable.\n",
        "    The goal is to find the best-fitting straight line that describes the relationship between these two variables.\n",
        "\n",
        "    2. Multiple Linear Regression:\n",
        "\n",
        "    This type of regression analyzes the relationship between two or more independent variables and a single dependent variable.\n",
        "    It aims to understand how these multiple predictors, taken together, influence the outcome.\n",
        "\n",
        "Understanding Simple Linear Regression vs Multiple Linear ...\n",
        "In essence, simple linear regression is a special case of multiple linear regression where the number of independent variables is limited to one."
      ],
      "metadata": {
        "id": "UhJ73v_87326"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        ">- The key assumptions of Multiple Linear Regression (MLR) are crucial for ensuring that the model produces reliable, unbiased, and interpretable results. Here are the core assumptions:\n",
        "\n",
        "    1. Linearity\n",
        "    The relationship between the dependent variable and each independent variable is linear.\n",
        "\n",
        "    This means the expected value of the dependent variable is a linear combination of the predictors.\n",
        "\n",
        "    2. Independence of Errors\n",
        "    The residuals (errors) are independent of each other.\n",
        "\n",
        "    There should be no autocorrelation, especially relevant in time-series data.\n",
        "\n",
        "    3. Homoscedasticity (Constant Variance of Errors)\n",
        "    The residuals have constant variance across all levels of the independent variables.\n",
        "\n",
        "    If the variance of the residuals changes (heteroscedasticity), it can lead to inefficient estimates.\n",
        "\n",
        "    4. Normality of Errors\n",
        "    The residuals should be approximately normally distributed, especially important for small sample sizes.\n",
        "\n",
        "    This is essential for valid hypothesis tests and confidence intervals.\n",
        "\n",
        "    5. No Multicollinearity\n",
        "    The independent variables should not be highly correlated with each other.\n",
        "\n",
        "    High multicollinearity can make it difficult to determine the individual effect of each predictor.\n",
        "\n",
        "    6. No Relevant Variables Omitted & No Irrelevant Variables Included\n",
        "    The model should be correctly specified, meaning all relevant variables are included, and irrelevant ones are excluded.\n",
        "\n",
        "    7. Measurement Accuracy\n",
        "    The independent and dependent variables are measured without significant measurement error."
      ],
      "metadata": {
        "id": "XRtd76hV8YqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        ">- Heteroscedasticity refers to a situation in regression analysis where the variance of the errors (residuals) is not constant across all levels of the independent variables. In other words, the \"spread\" or \"scatter\" of the residuals changes depending on the value of the predictors.\n",
        "\n",
        "In a Multiple Linear Regression (MLR) model, one of the key assumptions is homoscedasticity, which means that:\n",
        "The variance of the error terms is constant across all levels of the independent variables.\n",
        "\n",
        "When this assumption is violated — i.e., when heteroscedasticity is present — it can affect the regression results in the following ways:\n",
        "\n",
        "    Effects of Heteroscedasticity on MLR Results:\n",
        "    Aspect\t                       Impact\n",
        "\n",
        "    Unbiased Coefficients\t        The estimated regression coefficients (β's) remain unbiased. Heteroscedasticity does not affect the point estimates of the coefficients.\n",
        "\n",
        "    Efficiency of Coefficients\t   The estimated coefficients become inefficient, meaning they no longer have the smallest possible variance (they are not Best Linear Unbiased Estimators - BLUE).\n",
        "\n",
        "    Standard Errors\t              The standard errors of the coefficients are incorrect, often underestimated or overestimated, leading to unreliable p-values and confidence intervals.\n",
        "\n",
        "    Hypothesis Testing\t           Inference based on t-tests, F-tests, and confidence intervals may be invalid. You may incorrectly conclude that predictors are statistically significant (Type I or II errors).\n",
        "\n",
        "    Prediction Intervals\t         Prediction intervals may be inaccurate, especially for certain ranges of the predictors where the variance of errors is larger.\n",
        "\n",
        "Detecting Heteroscedasticity:\n",
        "\n",
        ">- Residual Plots: Plot residuals vs. fitted values; a funnel or cone shape suggests heteroscedasticity.\n",
        "\n",
        "Breusch-Pagan Test:\n",
        "\n",
        ">- Formal statistical test for heteroscedasticity.\n",
        "\n",
        "White's Test:\n",
        "\n",
        ">- Another common test that doesn't assume a specific form of heteroscedasticity.\n",
        "\n",
        "Addressing Heteroscedasticity:\n",
        "\n",
        ">- Transforming the Dependent Variable: e.g., log transformation can stabilize variance.\n",
        "\n",
        ">- Weighted Least Squares (WLS): Gives different weights to different observations to correct for non-constant variance.\n",
        "\n",
        ">- Robust Standard Errors (e.g., Huber-White): Adjusts the standard errors to be valid even in the presence of heteroscedasticity.\n",
        "\n",
        "Summary:\n",
        "Heteroscedasticity undermines the reliability of hypothesis testing and confidence intervals in MLR but does not bias the estimated coefficients themselves. Detecting and correcting for heteroscedasticity is essential to ensure valid statistical inference."
      ],
      "metadata": {
        "id": "5Rk66S1a8zYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        ">- To improve a Multiple Linear Regression (MLR) model with high multicollinearity, you can apply several strategies to reduce the negative impact of correlated predictors. Here's a clear, step-by-step breakdown of practical techniques:\n",
        "\n",
        "    1. Remove Highly Correlated Predictors\n",
        "\n",
        "    Use the correlation matrix or Variance Inflation Factor (VIF) to identify predictors with strong correlations.\n",
        "\n",
        "    Drop one of the variables from each highly correlated pair or group.\n",
        "\n",
        "    Example:\n",
        "    If X1 and X2 have a correlation coefficient > 0.8, consider removing one.\n",
        "\n",
        "\n",
        "\n",
        "    2. Combine Correlated Variables\n",
        "\n",
        "    Create a composite variable through Principal Component Analysis (PCA) or Factor Analysis, which transforms the correlated variables into uncorrelated components.\n",
        "\n",
        "    This reduces dimensionality while retaining most of the information.\n",
        "\n",
        "\n",
        "\n",
        "    3. Use Regularization Techniques\n",
        "\n",
        "    Apply regression methods that can handle multicollinearity by penalizing large coefficients:\n",
        "\n",
        "    >- 1. Ridge Regression (L2 regularization)\n",
        "\n",
        "    Shrinks coefficients but keeps all predictors.\n",
        "\n",
        "    Good if you want to retain all features but control multicollinearity.\n",
        "\n",
        "    >- 2. Lasso Regression (L1 regularization)\n",
        "\n",
        "    Performs feature selection by driving some coefficients to zero.\n",
        "\n",
        "    Useful if you want a simpler model and automatic variable selection.\n",
        "\n",
        "\n",
        "\n",
        "    4. Center or Standardize Predictors\n",
        "\n",
        "    Especially when dealing with interaction terms or polynomial features, centering (subtracting the mean) can reduce multicollinearity.\n",
        "\n",
        "\n",
        "\n",
        "    5. Collect More Data\n",
        "\n",
        "    Sometimes, multicollinearity is a symptom of a small dataset relative to the number of predictors.\n",
        "\n",
        "    More data can improve stability and reduce variance inflation.\n",
        "\n",
        "\n",
        "\n",
        "    6. Domain Knowledge for Feature Selection\n",
        "\n",
        "    Use your understanding of the problem to exclude redundant or less relevant variables, even if purely statistical methods don't flag them clearly.\n",
        "\n",
        "\n",
        "\n",
        "    7. Monitor Variance Inflation Factor (VIF)\n",
        "\n",
        "    Continuously assess VIF values during model building:\n",
        "\n",
        "    VIF > 5 or 10 typically signals problematic multicollinearity."
      ],
      "metadata": {
        "id": "5xQ5XMqL995p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        ">- Transforming categorical variables is a key preprocessing step in regression models, especially since most regression algorithms expect numerical input. Here are some common techniques used to convert categorical variables into a numerical format suitable for regression:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "How it works: Creates a new binary (0/1) column for each category.\n",
        "\n",
        "Best for: Nominal (unordered) categories with a relatively small number of unique values.\n",
        "\n",
        "Tools: pandas.get_dummies(), sklearn.preprocessing.OneHotEncoder\n",
        "\n",
        "    Example:\n",
        "\n",
        "\n",
        "    # Category: Color = ['Red', 'Blue', 'Green']\n",
        "    # Becomes:\n",
        "    # Red  Blue  Green\n",
        "    # 1     0     0\n",
        "    # 0     1     0\n",
        "    # 0     0     1\n",
        "\n",
        "\n",
        "\n",
        "2. Label Encoding\n",
        "How it works: Assigns a unique integer to each category.\n",
        "\n",
        "Best for: Ordinal variables (where order matters).\n",
        "\n",
        "Caution: Can mislead models into thinking there's a ranking for nominal variables.\n",
        "\n",
        "    Example:\n",
        "\n",
        "\n",
        "    # Size = ['Small', 'Medium', 'Large']\n",
        "    # Becomes: [0, 1, 2]\n",
        "\n",
        "\n",
        "\n",
        "3. Target (Mean) Encoding\n",
        "How it works: Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Best for: High-cardinality categorical features.\n",
        "\n",
        "Caution: Can lead to overfitting; should be used with regularization or cross-validation.\n",
        "\n",
        "    Example:\n",
        "\n",
        "\n",
        "    # If 'City' has average house prices:\n",
        "    # New York: 500k, Boston: 400k → Replace 'City' with 500,000 or 400,000\n",
        "\n",
        "\n",
        "\n",
        "4. Frequency (Count) Encoding\n",
        "How it works: Replace each category with the frequency or count of that category in the dataset.\n",
        "\n",
        "Best for: High-cardinality categorical variables, quick and simple method.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    # Job Title = ['Engineer', 'Doctor', 'Engineer', 'Lawyer']\n",
        "    # Becomes: [2, 1, 2, 1]\n",
        "\n",
        "\n",
        "\n",
        "5. Binary Encoding\n",
        "How it works: Converts categories to integers, then to binary format. Each binary digit gets its own column.\n",
        "\n",
        "Best for: High-cardinality categorical variables, balances dimensionality and information.\n",
        "\n",
        "Tool: category_encoders library in Python.\n",
        "\n",
        "\n",
        "\n",
        "6. Ordinal Encoding\n",
        "How it works: Map categories to integers based on their logical order.\n",
        "\n",
        "Best for: Ordered categories (e.g., education level, satisfaction ratings).\n",
        "\n",
        "Custom mapping is usually needed.\n",
        "\n",
        "    Example:\n",
        "\n",
        "\n",
        "    # Education: ['High School', 'Bachelors', 'Masters', 'PhD']\n",
        "    # Becomes: [1, 2, 3, 4]\n",
        "\n",
        "\n",
        "\n",
        "7. Hash Encoding\n",
        "How it works: Hashes categories into a fixed number of columns.\n",
        "\n",
        "Best for: Very high-cardinality features when memory or dimensionality is a concern.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sg0ptN3a--hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        ">- In multiple linear regression, interaction terms represent the joint effect of two or more independent variables on the dependent variable. They allow the relationship between one independent variable and the outcome to change depending on the value of another independent variable. Essentially, they model how the impact of one predictor variable varies based on the level of another predictor."
      ],
      "metadata": {
        "id": "SBhHKUHADThC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        ">- In simple linear regression, the intercept represents the predicted value of the dependent variable when the independent variable is zero. In multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero. Essentially, the interpretation remains the same – the starting point of the regression line or plane – but in multiple regression, it's conditional on all other predictors being zero."
      ],
      "metadata": {
        "id": "M9DLxMmADfkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. - What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        ">- In regression analysis, the slope of the regression line is crucial because it represents the rate of change of the dependent variable (the one being predicted) for each unit change in the independent variable (the predictor). A steeper slope indicates a greater impact of the independent variable on the dependent variable. The slope's value and sign (positive or negative) directly influence predictions: a positive slope means a positive relationship (as one variable increases, the other increases), while a negative slope indicates an inverse relationship (as one increases, the other decreases).\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "    Significance of the Slope:\n",
        "\n",
        "    Rate of Change:\n",
        "\n",
        "    The slope (often denoted as 'm' in the equation y = mx + b) quantifies how much the dependent variable is expected to change for every one-unit increase in the independent variable.\n",
        "\n",
        "    Direction of Relationship:\n",
        "\n",
        "    The sign of the slope (positive or negative) reveals the direction of the relationship between the variables.\n",
        "\n",
        "    Strength of Relationship:\n",
        "\n",
        "    A steeper slope generally indicates a stronger relationship, meaning the independent variable has a greater influence on the dependent variable.\n",
        "\n",
        "    Prediction:\n",
        "\n",
        "    The slope is used to make predictions about the dependent variable based on the independent variable. For example, if the slope is 2, you would predict a 2-unit increase in the dependent variable for every 1-unit increase in the independent variable.\n",
        "\n",
        "    How the Slope Affects Prediction:\n",
        "\n",
        "    Positive Slope:\n",
        "\n",
        "    If the slope is positive, the predicted value of the dependent variable will increase as the independent variable increases. For example, if you are predicting house prices based on square footage, a positive slope would mean that larger houses (with more square footage) are predicted to have higher prices.\n",
        "\n",
        "    Negative Slope:\n",
        "\n",
        "    If the slope is negative, the predicted value of the dependent variable will decrease as the independent variable increases. For example, if you are predicting the number of customers at a store based on the price of an item, a negative slope would mean that higher prices are predicted to result in fewer customers.\n",
        "\n",
        "    Steeper Slope:\n",
        "\n",
        "    A steeper slope (either positive or negative) means that changes in the independent variable have a more significant impact on the predicted values of the dependent variable.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    Let's say you're using linear regression to predict a student's final exam score (dependent variable) based on the number of hours they studied (independent variable).\n",
        "\n",
        "If the slope is 5, it means that for each additional hour of study, the predicted final exam score increases by 5 points.\n",
        "\n",
        "If the slope is -2, it means that for each additional hour of study, the predicted final exam score decreases by 2 points. This is less likely in a study scenario, but it demonstrates the concept.\n",
        "\n",
        "In essence, the slope is a key component of the regression equation and is essential for understanding and predicting the relationship between variables"
      ],
      "metadata": {
        "id": "2jmmiklMDopE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        ">- In a regression model, the intercept provides the predicted value of the dependent variable when all independent variables are zero. It essentially represents the starting point of the regression line on the y-axis. Understanding the intercept's context involves considering whether a zero value for the independent variable is meaningful in the real world and whether it falls within the range of the observed data.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "    Definition:\n",
        "\n",
        "    The intercept, often denoted as 'b' or 'a' in the equation y = a + bx (or y = b + mx), is the y-coordinate where the regression line crosses the y-axis.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "    It's the predicted value of the dependent variable when all independent variables are zero.\n",
        "\n",
        "Contextual Relevance:\n",
        "\n",
        "    Meaningful Zero: If a zero value for the independent variable is realistic and within the range of the data, the intercept can provide valuable context. For example, in a model predicting sales based on advertising spend, the intercept could represent baseline sales with no advertising.\n",
        "\n",
        "    Meaningless Zero: If a zero value for the independent variable is impossible or doesn't make sense (e.g., age of a person), the intercept might not have a direct real-world interpretation. In such cases, it's more of a mathematical construct, but still crucial for defining the regression line.\n",
        "\n",
        "    Extrapolation: The intercept is often used as a starting point for predictions, especially when extrapolating beyond the observed data range. However, it's crucial to be cautious when extrapolating, as the relationship between variables might change outside the observed range.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    If you have a regression model predicting house prices based on size, the intercept might represent the base price of a house with zero square footage. While a house with zero square footage is not realistic, the intercept is still useful for understanding the overall relationship between size and price.\n",
        "\n",
        "Importance:\n",
        "\n",
        "    The intercept is a critical component of the regression equation and plays a key role in understanding the relationship between variables."
      ],
      "metadata": {
        "id": "WSOSALl0EZAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        ">- R-squared, or the coefficient of determination, is a commonly used metric to evaluate the performance of regression models. However, it has several limitations that make it unsuitable as a sole measure of model performance. R-squared only indicates the proportion of variance explained, not necessarily how well the model fits the data, whether it's biased, or if it will generalize well to new data.\n",
        "\n",
        "Here's a more detailed breakdown of the limitations:\n",
        "\n",
        "    1. Sensitivity to Model Complexity and Overfitting:\n",
        "\n",
        "    R-squared increases as more variables are added to the model, even if those variables are not truly relevant. This can lead to overfitting, where the model captures noise in the training data rather than the underlying signal, resulting in poor performance on new data.\n",
        "\n",
        "    A high R-squared doesn't guarantee a good model, especially if the model is overly complex.\n",
        "\n",
        "\n",
        "\n",
        "    2. Insensitivity to Model Fit and Prediction Accuracy:\n",
        "\n",
        "    R-squared doesn't directly address the magnitude of prediction errors. A high R-squared can be achieved with a model that still has significant prediction errors.\n",
        "\n",
        "    It doesn't tell you how far off the model's predictions are in absolute terms, which is crucial for understanding the practical usefulness of the model.\n",
        "\n",
        "\n",
        "\n",
        "    3. Not Suitable for Non-Linear Relationships:\n",
        "\n",
        "    R-squared is based on linear decomposition of variance. In cases where the true relationship is non-linear, R-squared can be misleading and underestimate the model's fit.\n",
        "\n",
        "\n",
        "\n",
        "    4. Sensitivity to Outliers:\n",
        "\n",
        "    Outliers can disproportionately influence the R-squared value, potentially distorting the assessment of the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "    5. Doesn't Indicate Causation or Bias:\n",
        "\n",
        "    A high R-squared doesn't prove a causal relationship between variables, nor does it indicate whether the model is biased.\n",
        "\n",
        "    It's possible to have a high R-squared with a model that is fundamentally flawed or produces biased predictions.\n",
        "\n",
        "\n",
        "\n",
        "    6. Not Directly Comparable Across Different Datasets:\n",
        "\n",
        "    R-squared values are not directly comparable between models trained on different datasets or with different dependent variables.\n",
        "\n",
        "\n",
        "\n",
        "    7. Inadequate for Model Selection and Validation:\n",
        "\n",
        "    R-squared should not be the sole basis for selecting a model or validating its performance.\n",
        "\n",
        "    It's crucial to use other metrics, such as RMSE, MAE, or cross-validation, alongside R-squared to get a comprehensive view of model performance.\n",
        "\n",
        "\n",
        "    - In conclusion\n",
        "\n",
        "    while R-squared is a useful metric for understanding the proportion of variance explained, it should not be relied upon as the sole measure of model performance due to its limitations in capturing model fit, prediction accuracy, and generalization ability. A comprehensive evaluation should include other metrics and validation techniques."
      ],
      "metadata": {
        "id": "O6uVjCxPE0Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        ">- A large standard error for a regression coefficient suggests high variability in the estimate of that coefficient. This means the estimated coefficient is likely to vary considerably across different samples from the same population, and there's a greater chance the true population value is far from the estimated value. Essentially, a large standard error indicates less precision and more uncertainty in the estimated effect of the predictor variable on the outcome.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "\n",
        "    - Standard Error as Variability:\n",
        "\n",
        "    The standard error of a regression coefficient (often denoted as SE(β)) quantifies the variability of that coefficient across different datasets that could be sampled from the same population.\n",
        "\n",
        "\n",
        "\n",
        "    - Small vs. Large Standard Error:\n",
        "\n",
        "    Small SE: Indicates a more precise estimate, suggesting that the coefficient is likely to be similar across different samples and close to the true population value.\n",
        "\n",
        "\n",
        "\n",
        "    - Large SE:\n",
        "\n",
        "    Indicates a less precise estimate, implying that the coefficient could vary significantly across different samples and might not be a reliable representation of the true population relationship.\n",
        "\n",
        "\n",
        "\n",
        "    Impact on Interpretation:\n",
        "\n",
        "    A large SE relative to the coefficient value suggests that the estimated effect of the predictor variable might not be statistically significant, even if the coefficient itself is non-zero.\n",
        "\n",
        "    It means there's a higher probability that the observed effect is due to random chance rather than a genuine relationship between the variables.\n",
        "\n",
        "Example:\n",
        "\n",
        "    If you're examining the relationship between advertising spend and sales, and the standard error for the advertising coefficient is large, it suggests that the true impact of advertising on sales could be substantially different from what your model estimates.\n",
        "\n",
        "In simpler terms, a large standard error means you should be less confident in your estimated coefficient's ability to represent the true relationship between the variables in the population."
      ],
      "metadata": {
        "id": "vNNGbSsKFfWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it.\n",
        "\n",
        ">- Heteroscedasticity, the unequal variance of residuals in a regression model, can be identified in residual plots by observing patterns like a funnel or cone shape, where the spread of residuals changes with predicted values. It's crucial to address it because it can lead to unreliable statistical inferences, including biased standard errors and inaccurate p-values, potentially causing incorrect conclusions about the significance of predictor variables.\n",
        "Identifying Heteroscedasticity in Residual Plots:\n",
        "\n",
        "\n",
        "    Residuals vs. Fitted Values Plot:\n",
        "\n",
        "    A common method is to plot the residuals against the predicted values. If the spread of the residuals is not constant (i.e., it widens or narrows as the predicted values change), it suggests heteroscedasticity.\n",
        "\n",
        "\n",
        "\n",
        "    Funnel or Cone Shape:\n",
        "\n",
        "    A characteristic pattern in the residual plot is a funnel or cone shape, indicating that the variance of residuals is not constant across the range of predicted values.\n",
        "\n",
        "\n",
        "\n",
        "    Increasing or Decreasing Spread:\n",
        "\n",
        "    If the residuals appear to spread out (fan out) or tighten up (cone in) as the fitted values increase, this is another indication of heteroscedasticity.\n",
        "\n",
        "\n",
        "\n",
        "    Why Addressing Heteroscedasticity is Important:\n",
        "\n",
        "\n",
        "    Biased Standard Errors:\n",
        "\n",
        "    Heteroscedasticity violates a key assumption of ordinary least squares (OLS) regression, which is that the variance of the error terms is constant. When this assumption is violated, the standard errors of the regression coefficients are often underestimated, leading to incorrect p-values and inflated Type I error rates.\n",
        "\n",
        "\n",
        "\n",
        "    Unreliable Inference:\n",
        "\n",
        "    Because of the biased standard errors, confidence intervals and hypothesis tests based on the model may be unreliable. It becomes more likely to incorrectly identify statistically significant relationships between variables.\n",
        "\n",
        "\n",
        "\n",
        "    Inefficient Estimates:\n",
        "\n",
        "    Heteroscedasticity can also lead to inefficient parameter estimates, meaning that the estimates of the regression coefficients may have larger variances than they would if the data were homoscedastic (constant variance).\n",
        "\n",
        "\n",
        "\n",
        "    Impact on Predictions:\n",
        "\n",
        "    Heteroscedasticity can also affect the accuracy of predictions made by the model, especially for observations with large residuals.\n",
        "\n",
        "- In summary :\n",
        "\n",
        " recognizing and addressing heteroscedasticity in residual plots is essential for ensuring the reliability and validity of regression analysis"
      ],
      "metadata": {
        "id": "qYlxxczSGAGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        ">- A high R² but low adjusted R² in a multiple linear regression model suggests that the model is likely overfitted or contains unnecessary predictor variables. R² measures the proportion of variance in the dependent variable explained by the model, and it always increases when more predictors are added, even if those predictors are not statistically significant. Adjusted R², however, penalizes the addition of irrelevant predictors, so a large discrepancy between R² and adjusted R² indicates that the model is fitting noise in the data rather than true relationships.\n",
        "\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "    R² (Coefficient of Determination):\n",
        "    Represents the proportion of variance in the dependent variable that is explained by the independent variables in the model. A higher R² generally indicates a better fit, but it doesn't account for model complexity.\n",
        "\n",
        "\n",
        "\n",
        "    Adjusted R²:\n",
        "    A modified version of R² that penalizes the addition of irrelevant independent variables. It increases only when an added predictor improves the model's fit beyond what would be expected by chance.\n",
        "\n",
        "\n",
        "\n",
        "    Overfitting:\n",
        "    When a model is too complex (e.g., too many predictors) and starts to fit the random noise in the training data, it leads to high R² but poor performance on new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "    Interpretation:\n",
        "    If R² is high (e.g., close to 1) and adjusted R² is significantly lower, it's a strong indication of overfitting or that some predictors are not contributing meaningfully to the model's predictive power.\n",
        "\n",
        "In essence, a large difference between R² and adjusted R² signals that the model may be capturing noise rather than real relationships, potentially leading to poor generalization to new data"
      ],
      "metadata": {
        "id": "WK17fFGxGkP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        ">- Scaling variables in multiple linear regression is crucial for several reasons: it speeds up model training by improving the convergence of gradient descent algorithms, it allows for fairer comparisons of feature importance, and it can prevent issues related to multicollinearity. Without scaling, features with larger magnitudes can disproportionately influence the model, potentially leading to inaccurate results and difficulty in interpreting coefficients.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "\n",
        "    1. Faster Convergence of Gradient Descent:\n",
        "\n",
        "    In multiple linear regression, optimization algorithms like gradient descent are often used to find the best-fitting model parameters (coefficients).\n",
        "\n",
        "    When features have vastly different scales (e.g., one feature measured in thousands and another in single digits), the optimization process can become inefficient and slow.\n",
        "\n",
        "    Scaling features to a similar range (e.g., using standardization or normalization) ensures that each feature contributes more evenly to the gradient calculation, allowing for faster convergence to the optimal solution.\n",
        "\n",
        "\n",
        "\n",
        "    2. Fairer Comparison of Feature Importance:\n",
        "\n",
        "    Without scaling, the coefficient of a feature with a larger scale will have a greater impact on the predicted outcome, even if it doesn't represent a more important relationship.\n",
        "\n",
        "    Scaling allows for a more accurate comparison of the relative importance of different features because the coefficients are now on a comparable scale.\n",
        "\n",
        "    For example, if you're predicting house prices and one feature is house size in square feet and another is the number of bedrooms, scaling these features will help you understand if size or number of bedrooms is a stronger predictor.\n",
        "\n",
        "\n",
        "\n",
        "    3. Avoiding Multicollinearity Issues:\n",
        "\n",
        "    Multicollinearity occurs when independent variables in a regression model are highly correlated with each other.\n",
        "\n",
        "    This can lead to unstable coefficient estimates, making it difficult to interpret the individual effects of each feature.\n",
        "\n",
        "    Scaling can help mitigate multicollinearity by reducing the correlation between features.\n",
        "\n",
        "\n",
        "\n",
        "    4. Improved Model Interpretability:\n",
        "\n",
        "    By scaling the variables, the coefficients obtained from the regression model can be more easily interpreted.\n",
        "\n",
        "    For example, if you scale your features to have a standard deviation of 1, the coefficients can be interpreted as the change in the dependent variable for a one standard deviation change in the corresponding independent variable.\n",
        "\n",
        "In summary, scaling variables in multiple linear regression is a crucial preprocessing step that enhances model performance, interpretability, and stability by addressing potential issues arising from features with differing scales"
      ],
      "metadata": {
        "id": "jnfyX3drHBoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        ">- Polynomial regression is a type of regression analysis used when the relationship between the independent and dependent variables is non-linear. It extends linear regression by including polynomial terms (e.g., x², x³, etc.) of the independent variable in the model. This allows the model to fit curves and other non-linear patterns in the data, making it more flexible than simple linear regression."
      ],
      "metadata": {
        "id": "bWtPIJUOHrN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        ">- Linear and polynomial regressions differ in the types of relationships they model. Linear regression assumes a straight-line relationship between variables, while polynomial regression can model curves by including polynomial terms (like x², x³, etc.) in the equation. This allows polynomial regression to fit more complex, non-linear relationships in the data."
      ],
      "metadata": {
        "id": "k2v7g6v_H3k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        ">- Polynomial regression is used when the relationship between the independent and dependent variables is non-linear and cannot be adequately represented by a straight line, as in linear regression. This means that if a scatter plot of your data shows a clear curve or a non-linear pattern, polynomial regression might be a good choice."
      ],
      "metadata": {
        "id": "CE6vDd6NIBbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        ">- The general equation for polynomial regression, when predicting a dependent variable y based on a single independent variable x, is: y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε.\n",
        "\n",
        "Here's a breakdown of the components:\n",
        "\n",
        "    y: The predicted value of the dependent variable.\n",
        "\n",
        "    x: The independent variable.\n",
        "\n",
        "    β₀: The intercept, or the value of y when x is zero.\n",
        "\n",
        "    β₁, β₂, ... βₙ: The coefficients for each power of x, representing the impact of each polynomial term on the prediction.\n",
        "\n",
        "    x², x³, ... xⁿ: The independent variable raised to different powers (quadratic, cubic, etc.).\n",
        "\n",
        "    ε: The error term, accounting for the variability in y that's not explained by the polynomial terms.\n",
        "\n",
        "    n: The degree of the polynomial (e.g., n=2 for a quadratic, n=3 for a cubic, etc.).\n",
        "\n",
        "In essence, polynomial regression extends linear regression by including higher-order terms of the independent variable to model non-linear relationships. The degree 'n' determines the complexity of the curve that can be fitted to the data."
      ],
      "metadata": {
        "id": "t0Aig_WiIJ03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        ">- Yes, polynomial regression can be applied to multiple independent variables. It allows you to model non-linear relationships between these variables and a dependent variable, capturing interactions between them as well. This is achieved by adding polynomial terms (like squared or cubed terms) and interaction terms (like the product of two variables) to the regression equation."
      ],
      "metadata": {
        "id": "BLjiWwP2IaNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "\n",
        ">- Polynomial regression, while versatile in modeling non-linear relationships, has limitations including overfitting, computational complexity, and difficulty in choosing the optimal polynomial degree. Overfitting, especially with high-degree polynomials, can lead to poor generalization on new data. Additionally, high-degree polynomials increase computational cost and make model selection challenging."
      ],
      "metadata": {
        "id": "z579aQCDIiOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        ">- o select the optimal degree of a polynomial in regression, several methods can be employed to evaluate model fit and avoid overfitting or underfitting. These include visual inspection, cross-validation, residual analysis, and comparing information criteria like AIC or BIC.\n",
        "Elaboration:\n",
        "\n",
        "    1. Visual Inspection:\n",
        "\n",
        "    Plotting the data and the polynomial regression curves for different degrees can provide a quick visual assessment of how well the model captures the underlying trend. A good fit should follow the general shape of the data without being overly complex.\n",
        "\n",
        "\n",
        "\n",
        "    2. Cross-validation:\n",
        "\n",
        "    This technique involves splitting the data into training and validation sets. The model is trained on the training set and evaluated on the validation set for different polynomial degrees. The degree that yields the best performance (e.g., lowest error) on the validation set is chosen.\n",
        "\n",
        "\n",
        "\n",
        "    3. Residual Analysis:\n",
        "\n",
        "    After fitting a polynomial model, examining the residuals (the differences between predicted and actual values) can reveal patterns. A well-fitted model should have residuals that are randomly scattered around zero. Systematic patterns in the residuals indicate that the model may not be capturing the true relationship in the data.\n",
        "\n",
        "\n",
        "\n",
        "    4. Information Criteria (AIC/BIC):\n",
        "\n",
        "    Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are statistical measures that balance model fit with model complexity. They penalize models with more parameters (higher degree polynomials) to avoid overfitting. Lower AIC/BIC values generally indicate a better model.\n",
        "\n",
        "\n",
        "\n",
        "    5. Forward and Backward Selection:\n",
        "\n",
        "    - Forward Selection: Start with a low-degree polynomial and iteratively increase the degree, evaluating the model fit at each step. Stop when adding more terms doesn't significantly improve the fit.\n",
        "\n",
        "    - Backward Selection: Begin with a high-degree polynomial and iteratively remove terms, assessing the model's fit at each step. Stop when removing terms significantly degrades the fit.\n",
        "\n",
        "\n",
        "\n",
        "    6. R-squared and other error metrics:\n",
        "\n",
        "\n",
        "    R-squared (coefficient of determination) measures the proportion of variance in the dependent variable that is explained by the model. Other error metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) can also be used to evaluate model fit."
      ],
      "metadata": {
        "id": "bLblioKyIvVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        ">- Visualization is crucial in polynomial regression because it helps in understanding the relationship between variables, assessing model fit, and identifying potential issues like overfitting. Visualizing the data and the fitted polynomial curve allows for a more intuitive grasp of how well the model captures the underlying patterns, especially when dealing with non-linear relationships.\n",
        "\n",
        "Here's why visualization is important:\n",
        "\n",
        "    Understanding Non-linear Relationships:\n",
        "\n",
        "    Polynomial regression is used when the relationship between variables is not linear. Visualizing the data points and the fitted curve helps to see if the polynomial model is effectively capturing the curvature and non-linear patterns.\n",
        "\n",
        "\n",
        "\n",
        "    Assessing Model Fit:\n",
        "\n",
        "    A scatter plot of the data points with the fitted polynomial curve allows for visual inspection of how well the curve aligns with the data. A good fit means the predicted values closely follow the actual data points.\n",
        "\n",
        "\n",
        "\n",
        "    Detecting Overfitting:\n",
        "\n",
        "    Overfitting occurs when the model fits the training data too closely, including noise or random fluctuations. Visualizing the model can reveal if the curve is excessively wiggly, indicating overfitting. In such cases, the model might perform poorly on new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "    Choosing the Right Degree:\n",
        "\n",
        "    Polynomial regression involves selecting a degree for the polynomial equation. Visualizing the model with different degrees can help in choosing the optimal degree that provides a good balance between bias and variance.\n",
        "\n",
        "\n",
        "\n",
        "    Identifying Outliers:\n",
        "\n",
        "    Outliers, which are data points significantly deviating from the general trend, can be easily identified when visualized. These outliers can heavily influence the polynomial regression model, and visualization helps in spotting them and potentially deciding on outlier handling strategies.\n",
        "\n",
        "\n",
        "\n",
        "    Comparing Models:\n",
        "\n",
        "    Visualizing the results of polynomial regression alongside other models, like linear regression, can help in comparing their performance. The plot can clearly show which model better represents the data.\n",
        "\n",
        "\n",
        "\n",
        "    Communication:\n",
        "\n",
        "    Visualizations make it easier to communicate the results of the analysis to others, even those without a strong statistical background. It allows for a clearer understanding of the model and its implications.\n",
        "\n",
        "In essence, visualization in polynomial regression is not just an optional step but a vital component for model building, evaluation, and interpretation."
      ],
      "metadata": {
        "id": "8G3sTXBiJOqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.  How is polynomial regression implemented in Python?\n",
        "\n",
        ">- Polynomial regression in Python is typically implemented using the scikit-learn library, extending the principles of linear regression to accommodate non-linear relationships.\n",
        "\n",
        "Steps for Implementation:\n",
        "\n",
        "Import Libraries: Import numpy for numerical operations, matplotlib.pyplot for visualization, PolynomialFeatures from sklearn.preprocessing to generate polynomial features, and LinearRegression from sklearn.linear_model for the regression model.\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "Prepare Data: Create or load your dataset, ensuring you have independent variables (features) and a dependent variable (target).\n",
        "\n",
        "\n",
        "    # Example: Generate synthetic data with a quadratic relationship\n",
        "    np.random.seed(0)\n",
        "    X = np.random.rand(100, 1) * 10\n",
        "    y = 2 + 3 * X + X**2 + np.random.randn(100, 1) * 10\n",
        "\n",
        "Transform to Polynomial Features: Use PolynomialFeatures to transform your input data into a new feature matrix containing polynomial terms up to a specified degree. This effectively creates new features that are powers of the original features.\n",
        "\n",
        "\n",
        "    degree = 2  # Example: for a quadratic relationship\n",
        "    poly_features = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "Fit Linear Regression Model: Apply a standard LinearRegression model to the transformed polynomial features. Despite fitting a non-linear relationship in the original feature space, the model remains linear in the polynomial feature space.\n",
        "\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "\n",
        "Make Predictions: Use the trained model to make predictions on new or existing data.\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_poly)\n",
        "\n",
        "Visualize Results (Optional but Recommended): Plot the original data points and the fitted polynomial curve to visually assess the model's performance.\n",
        "\n",
        "\n",
        "    plt.scatter(X, y, color='blue', label='Data Points')\n",
        "    # Sort X for a smooth plot of the fitted curve\n",
        "    X_sorted = np.sort(X, axis=0)\n",
        "    plt.plot(X_sorted, model.predict(poly_features.transform(X_sorted)), color='red', label='Polynomial Regression')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Polynomial Regression Fit')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iMKSx9RKKGar"
      }
    }
  ]
}