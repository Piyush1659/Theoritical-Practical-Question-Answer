{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Questions\n"
      ],
      "metadata": {
        "id": "GTtid_Ouv5mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a parameter?\n",
        "\n",
        "- A parameter in theoretical terms refers to:\n",
        "\n",
        ">- A variable or constant that characterises or controls a system, mathematical expression, or model but is not the main variable of interest.\n",
        "\n",
        "In more depth:\n",
        "\n",
        "    Mathematical context:\n",
        "A parameter might be a number or symbol that specifies a family of functions or equations.\n",
        "\n",
        "    Statistical context:\n",
        "A parameter refers to a numerical characteristic of a population (like its mean or standard deviation) that we typically want to estimate based on a sample.\n",
        "\n",
        "    General or theoretical context:\n",
        "A parameter sets conditions or controls under which a process operates, or it guides the form a theory might take."
      ],
      "metadata": {
        "id": "hYmjz8YGv81j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "Correlation refers to a statistical measure that shows the relationship between two variables — whether and how strongly they move together.\n",
        "\n",
        ">- Positive correlation means both variables move in the same direction. If one increases, the other increases; if one decreases, the other decreases.\n",
        "Example: Height and weight — taller people tend to weigh more.\n",
        "\n",
        ">- Negative correlation means the two variables move in opposite directions. If one increases, the other decreases, or vice versa.\n",
        "Example: Speed and travel time — the faster you drive, the less time it takes to reach your destination.\n",
        "\n",
        "The strength of this relationship is typically measured by Pearson’s correlation coefficient (r), which ranges from -1 (perfect negative) to +1 (perfect positive), with 0 indicating no correlation."
      ],
      "metadata": {
        "id": "qXNPPZOHwZre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        ">- Machine Learning (ML) refers to the process by which computers learn from data and improve their performance without being explicitly programmed with hardcoded instructions.\n",
        "Instead of following step-by-step instructions, ML algorithms identify patterns and relationships within the data, allowing them to make predictions, decisions, or perform tasks with growing accuracy over time.\n",
        "\n",
        "Machine Learning typically involves the following main components:\n",
        "\n",
        "    Data  \n",
        "\n",
        "The raw information used to train and evaluate the algorithm (examples, samples, training set, test set).\n",
        "\n",
        "    Model\n",
        "\n",
        "The mathematical representation or algorithm that parses the data and makes predictions (like decision trees, neural networks, or regression models).\n",
        "\n",
        "    Training Algorithm\n",
        "\n",
        "The procedure or method by which the model is trained and updated (such as gradient descent).\n",
        "\n",
        "    Objective/ Loss Func­tion\n",
        "\n",
        "A metric used to measure how well the model performs (error, accuracy, cross-entropy) — guiding its improvement during training.\n",
        "\n",
        "    Evaluation/ Validation\n",
        "\n",
        "Techniques (like cross-validation or a separate test set) used to gauge the model’s ability to generalize to new, previously unseen data.\n",
        "\n",
        "    Predictions/ Inference\n",
        "\n",
        "The process by which the trained model applies its knowledge to make predictions or decisions on new inputs."
      ],
      "metadata": {
        "id": "uiET-6ZHwshA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value is a key metric for evaluating how well your model is performing during training (and afterwards, on validation or test data). Here’s how it helps:\n",
        "\n",
        "    What loss measures:\n",
        "The loss quantifies the error between your model’s predictions and the true values.\n",
        "\n",
        ">- A lower loss means your predictions are close to the ground truth.\n",
        "\n",
        ">- A higher loss signals greater error.\n",
        "\n",
        "    Determining if a model is good or not:\n",
        "\n",
        ">- If the loss drops and stabilises at a low value during training, it typically means your model is learning well.\n",
        "\n",
        ">- If the loss plateaus or increases, it might be a sign of problems, such as underfitting, overfitting, or a poor architecture.\n",
        "\n",
        "    Training vs Validation Loss:\n",
        "Comparing training and validation loss is crucial:\n",
        "\n",
        ">- If training and validation lose are both high, your model might be underfitting.\n",
        "\n",
        ">- If training loss is low but validation loss is high, your model might be overfitting (memorizing instead of generalizing).\n",
        "\n",
        ">- If both are low and close to each other, your model is likely performing well.\n",
        "\n",
        "    Other consideration:\n",
        "While the loss is a helpful indicator, it's not the whole story. Often, you will want to combine it with other metrics (like accuracy, precision, recall, or F1 score) and perform additional diagnostics (confusion matrix, ROC curve, etc.)."
      ],
      "metadata": {
        "id": "iOzDEOpAxBvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "    Continuous variables:\n",
        "These can take any numerical value within a range — even fractions or decimal points.\n",
        "Examples include:\n",
        "\n",
        "Height (e.g. 160.5 cm)\n",
        "\n",
        "Weight (e.g. 72.3 kg)\n",
        "\n",
        "Time (e.g. 2.7 hours)\n",
        "\n",
        "Distance (e.g. 5.5 km)\n",
        "\n",
        "    Categorical variables:\n",
        "These can take a limited number of separate, non-numeric values or categories.\n",
        "Examples include:\n",
        "\n",
        "Gender (e.g. male, female)\n",
        "\n",
        "Eye color (e.g. blue, green, brown)\n",
        "\n",
        "Marital status (e.g. single, married, divorced)\n",
        "\n",
        "Type of vehicle (e.g. car, bike, truck)"
      ],
      "metadata": {
        "id": "i76O7dFKxaXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        ">- Categorical variables need special treatment because ML models typically work with numerical data. Here are some common techniques to handle categorical variables:\n",
        "\n",
        "    Label Encoding\n",
        ">- What: Encodes each category with a unique integer.\n",
        "\n",
        ">- When to use: If there’s ordinal significance (like low, medium, high).\n",
        "\n",
        ">- Pros: Simple; maintains order if applicable.\n",
        "\n",
        ">- Cons: May confuse the algorithm if there’s no real ordering (integer might be interpreted as implying a numerical relation).\n",
        "   \n",
        "    One-Hot Encoding (Dummy variables)\n",
        ">- What: Transforms each category into a new column with 0 or 1.\n",
        "\n",
        ">- When to use: If there’s no ordinal relation.\n",
        "\n",
        ">- Pros: Prevents algorithm from interpreting numerical relationships.\n",
        "\n",
        ">- Cons: Increases dimensionality (especially with many unique categories).\n",
        "\n",
        "    Target / Mean Encoding (with caution)\n",
        ">- What: Encodes each category by the mean of the target variable for that category.\n",
        "\n",
        ">- When to use: Large number of categories with high cardinality.\n",
        "\n",
        ">- Pros: Reduce dimensionality while retaining information.\n",
        "\n",
        ">- Cons: Higher risk of data leaks or overfitting if not done carefully (should be done within cross-validation).\n",
        "\n",
        "    Binary/Hash Encoding (for high cardinality)\n",
        ">- Encodes each category into a fixed number of bits or hashed components.\n",
        "\n",
        ">- Helps reduce dimensionality while retaining distinctions.'\n",
        "\n"
      ],
      "metadata": {
        "id": "si9rt1FZxj02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What do you mean by training and testing a dataset.\n",
        "\n",
        "When you train and test a dataset, you’re following a standard procedure to make sure your algorithm performs well and can generalize to new, unseen data.\n",
        "\n",
        "Here’s a breakdown:\n",
        "\n",
        "    Training a dataset:\n",
        "\n",
        ">- This is the portion of your data used to teach or “train” your algorithm.\n",
        "\n",
        ">- The algorithm looks for patterns, relationships, and structures within this data.\n",
        "\n",
        ">- For example: If you’re training a classifier to identify cats and dogs, your training set includes many labeled photos of cats and dogs.\n",
        "\n",
        "    Testing a dataset:\n",
        "\n",
        ">- This is a separate set of data that’s withheld from training.\n",
        "\n",
        ">- After training, you feed this new, previously unseen data into your algorithm.\n",
        "\n",
        ">- The algorithm’s performance here lets you gauge its ability to generalize — whether it performs well with new inputs instead of just memorizing the training data."
      ],
      "metadata": {
        "id": "BRvk2TyNyCGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is sklearn.preprocessing?\n",
        "\n",
        ">- sklearn.preprocessing refers to a module in scikit-learn (sklearn) that provides utilities for transforming raw data into a format that's more suitable for machine learning models.\n",
        "\n",
        "This typically involves scaling, normalizing, or encoding the data."
      ],
      "metadata": {
        "id": "muS8mMo9yVkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is a Test set?\n",
        "\n",
        ">- A test set refers to a collection of data that is kept aside and not used during the training process of a machine learning model. Instead, it’s used afterwards to evaluate the model’s performance in a realistic scenario — that is, on data it’s never seen before."
      ],
      "metadata": {
        "id": "6UF-ofA_LIrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        ">- Usually, we use train test split from sklearn.model selection. For example:\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # data in X (features) and y (labels)\n",
        "    X = [...]\n",
        "    y = [...]\n",
        "\n",
        "    # Split into training and testing sets\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,   # 20% for testing\n",
        "    random_state=42   # for reproducibility\n",
        "    )\n",
        "\n",
        "Here’s a standard approach to tackling a Machine Learning problem:\n",
        "\n",
        "1. Understand the problem.\n",
        "\n",
        ">- What kind of problem is it? (Regression, Classification, Clustering, etc.)\n",
        "\n",
        ">- What’s the business objective?\n",
        "\n",
        "2. Gather and prepare the data.\n",
        "\n",
        ">- Acquire the data (CSV, database, API).\n",
        "\n",
        ">- Handle missing values, duplicates, and inconsistencies.\n",
        "\n",
        ">- Perform exploratory data analysis (EDA) — visualize, compute statistics, and identify patterns.\n",
        "\n",
        "3. Feature engineering.\n",
        "\n",
        ">- Select or create relevant features.\n",
        "\n",
        ">- Transform or scale if needed (normalize, standardize).\n",
        "\n",
        ">- Encode categorical variables.\n",
        "\n",
        "4. Split the data.\n",
        "\n",
        ">- Split into training, validation, and/or testing sets.\n",
        "\n",
        "5. Choose and train a model.\n",
        "\n",
        ">- Select algorithm(s).\n",
        "\n",
        ">- Train the algorithm(s) on training data.\n",
        "\n",
        "6. Evaluate the model.\n",
        "\n",
        ">- Use appropriate metrics (accuracy, RMSE, F1-score).\n",
        "\n",
        ">- Validate against a separate test set or through cross-validation.\n",
        "\n",
        "7. Hyperparameter Tuning (if needed).\n",
        "\n",
        ">- Use grid search, random search, or Bayesian optimization.\n",
        "\n",
        "8. Finalize and deploy.\n",
        "\n",
        ">- Prepare for production.\n",
        "\n",
        ">- Monitor performance over time."
      ],
      "metadata": {
        "id": "01Km4EyPLTdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        ">- We perform Exploratory Data Analysis (EDA) before fitting a model for several key reasons:\n",
        "\n",
        "    1. Understand the data’s structure and relationships\n",
        "\n",
        ">- EDA helps us discover:\n",
        "\n",
        ">- The distribution of each variable\n",
        "\n",
        ">- Relationships or correlations between variables\n",
        "\n",
        ">- The presence of outliers or anomalies\n",
        "\n",
        ">- Patterns or clusters in the data\n",
        "\n",
        "    2. Identify data issues and prepare for modeling\n",
        "\n",
        ">- Before we feed the data into a model, we need to make sure it’s “clean” and appropriate for the algorithm. EDA lets us:\n",
        "\n",
        ">- Handle missing values\n",
        "\n",
        ">- Detect and deal with duplicates\n",
        "\n",
        ">- Transform variables if needed (such as scaling or normalizing)\n",
        "\n",
        ">- Encode categorical variables\n",
        "\n",
        ">- Remove or address outliers\n",
        "\n",
        "    3. Improve Model Performance and Interpretability\n",
        "\n",
        ">- Using the knowledge gained from EDA, we can:\n",
        "\n",
        ">- Select the most relevant features\n",
        "\n",
        ">- Reduce dimensionality if there’s redundancy\n",
        "\n",
        ">- Apply appropriate transformation to aid algorithm performance\n",
        "\n",
        ">- Provide context for interpreting the eventual results\n",
        "\n",
        "    4. Avoid “Garbage In, Garbage Out”\n",
        "\n",
        ">- If we feed poor or misunderstood data into a model, the algorithm’s output will reflect those problems — which can undermine its credibility and utility."
      ],
      "metadata": {
        "id": "9BI6ppNNOCom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. What is correlation?\n",
        "\n",
        ">- Correlation refers to the statistical relationship or association between two or more variables.\n",
        "\n",
        "    Key points about correlation:\n",
        "\n",
        ">- Direction: It can be positive, negative, or zero (no correlation).\n",
        "\n",
        ">- Strength: Measured by the correlation coefficient (r), which ranges from -1 to +1"
      ],
      "metadata": {
        "id": "XKwGFWwOOhuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. What does negative correlation mean?\n",
        "\n",
        ">- A negative correlation means that two variables move in ** opposite directions**.\n",
        "\n",
        "    When one increases, the other decreases, or vice versa.\n",
        "\n",
        "- Picture these examples:\n",
        "\n",
        ">- The faster you drive, the less time it takes to reach your destination (negative correlation).\n",
        ">- As stress increases, quality of sleep often drops (negative correlation).\n",
        ">- An increase in exercise might be related to a decrease in weight (negative correlation).\n",
        "\n",
        "    The strength of a negative correlation is measured by a correlation coefficient (r) that falls between -1 and 0:\n",
        "\n",
        ">- -1 = perfect negative correlation (they move in exact opposition).\n",
        "\n",
        ">- -0.5 = medium or strong but not perfect.\n",
        "\n",
        ">- 0 = no correlation at a"
      ],
      "metadata": {
        "id": "gQ0Qok-AOvEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. How can you find correlation between variables in Python?\n",
        "\n",
        ">- You can find the correlation between variables in Python using Pandas’ corr() method or NumPy’s corrcoef()"
      ],
      "metadata": {
        "id": "lW7YjntQPEtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        ">- Causation refers to a cause-and-effect relationship, where one event directly produces or brings about another event.\n",
        "\n",
        ">- Correlation, meanwhile, means two events are related or move together, but this doesn’t necessarily indicate that one directly causes the other.\n",
        "\n",
        "    Example:\n",
        "\n",
        ">- Let's say we observe these two facts:\n",
        "\n",
        ">- The number of ice cream sales increases.\n",
        "\n",
        ">- The number of drowning accidents also increases at the same time."
      ],
      "metadata": {
        "id": "RQWSkn1sPYi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        ">- An optimizer is an algorithm or method used to adjust the parameters of a model (typically weights and biases in a neural network) to minimize a objective or loss function.\n",
        "Essentially, the optimizer guides the training process by following the slope (gradient) of the loss to find a minimum — hopefully the minimum that results in the best performance.\n",
        "\n",
        ">- Types of Optimizers (with Examples)\n",
        "\n",
        "1. Gradient Descent (GD) or Batch Gradient Descent\n",
        "Description: Computes the gradient of the entire training set to perform a single update.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    for epoch in range(epochs):\n",
        "        gradient = compute_full_batch_gradient(model, training_data)\n",
        "        model.weights -= learning_rate * gradient\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Description: Computes the gradient for a single training example or a small mini-batch instead of the entire dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in training_data:\n",
        "           gradient = compute_gradient(model, x, y)\n",
        "           model.weights -= learning_rate * gradient\n",
        "\n",
        "3. Mini-batch Gradient Descent\n",
        "Description: Combine the benefits of both methods — compute the gradient on a mini-batch of samples.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    batch_size = 32\n",
        "    for epoch in range(epochs):\n",
        "        for batch in batches(training_data, batch_size):\n",
        "            gradient = compute_batch_gradient(model, batch)\n",
        "            model.weights -= learning_rate * gradient\n",
        "\n",
        "\n",
        "4. Momentum SGD\n",
        "Description: Accumulates momentum from previous updates, adding a fraction of the previous update to accelerate convergence.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    v = 0\n",
        "    beta = 0.9\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in training_data:\n",
        "            gradient = compute_gradient(model, x, y)\n",
        "            v = beta * v + gradient\n",
        "            model.weights -= learning_rate * v\n",
        "\n",
        "\n",
        "5. Adagrad (Adaptive Gradient Algorithm)\n",
        "Description: Adapts the learning rate for each parameter based on its historical squared gradient.       \n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    G = 0\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in training_data:\n",
        "            gradient = compute_gradient(model, x, y)\n",
        "            G += gradient**2\n",
        "            model.weights -= learning_rate * gradient / (np.sqrt(G) + epsilon)\n",
        "\n",
        "\n",
        "6. RMSProp (Root Mean Square Propagation)\n",
        "Description: Similar to Adagrad but with exponentially decayed average of squared gradients instead of sum.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    G = 0\n",
        "    beta = 0.9\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in training_data:\n",
        "            gradient = compute_gradient(model, x, y)\n",
        "            G = beta * G + (1 - beta) * gradient**2\n",
        "            model.weights -= learning_rate * gradient / (np.sqrt(G) + epsilon)\n",
        "\n",
        "\n",
        "\n",
        ". Adam (Adaptive Moment Estimation)\n",
        "Description: Combines momentum and RMSProp.\n",
        "\n",
        "Example:\n",
        "\n",
        "    # Pseudo-code\n",
        "    v = 0\n",
        "    G = 0\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    epsilon = 1e-8\n",
        "    t = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in training_data:\n",
        "            t += 1\n",
        "            gradient = compute_gradient(model, x, y)\n",
        "            v = beta1 * v + (1 - beta1) * gradient\n",
        "            G = beta2 * G + (1 - beta2) * (gradient**2)\n",
        "            v_corrected = v / (1 - beta1**t)\n",
        "            G_corrected = G / (1 - beta2**t)\n",
        "            model.weights -= learning_rate * v_corrected / (np.sqrt(G_corrected) + epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lf55WCMWPmcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. What is sklearn.linear_model ?\n",
        "\n",
        ">- sklearn.linear_model is a module within scikit-learn (sklearn) — a popular machine learning library in Python — that contains a collection of classes and functions for linear models.\n",
        "\n",
        "- What are linear models?\n",
        ">-  Linear models are methods for understanding or predicting a variable (the output or target) as a linear combination of other variables (the inputs or features)."
      ],
      "metadata": {
        "id": "IiKVitF4Rqfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        ">- Forward pass: Computes predictions for each training example.\n",
        "\n",
        ">- Loss calculation: Measures the error by comparing predictions to true values.\n",
        "\n",
        ">- Backward pass (backpropagation): Computes the gradient of the loss with respect to each parameter.\n",
        "\n",
        ">- Weight update: Adjusts the parameters in the direction that reduces the loss.\n",
        "\n",
        "# Arguments:\n",
        "\n",
        ">- epochs (integer): Number of times to iterate over the training data.\n",
        "\n",
        ">- batch_size (integer): Number of samples per update.\n",
        "\n",
        ">- validation_data (tuple): Validation inputs and targets (for evaluating performance after each epoch).\n",
        "\n",
        ">- shuffle (boolean): Whether to shuffle training samples each epoch.\n",
        "\n",
        ">- callbacks (list): List of callbacks (like ModelCheckpoint, EarlyStopping) to aid training.\n",
        "\n",
        ">- steps_per_epoch: Number of batches to process in each epoch (typically used with generators).\n",
        "\n"
      ],
      "metadata": {
        "id": "NXBog_AFR5ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        ">- The model.predict() method is a way to generate predictions or outputs from a trained machine learning or deep learning model.\n",
        "\n",
        "# What it does:\n",
        "It takes in new, unseen data (typically called input samples) and produces a prediction — for instance:\n",
        "\n",
        ">- Probabilities for each class in a classifier\n",
        "\n",
        ">- Continuous values for regression\n",
        "\n",
        ">- Encoded output for a generative model, etc.\n",
        "\n",
        "# What arguments are typically required?\n",
        "\n",
        "The most essential and typically required argument is:\n",
        "\n",
        ">- The input data — normally called X.\n",
        "\n",
        "This should be in a format that your model expects (for instance, a NumPy array, a Tensor, or a DataFrame), with the appropriate dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "MLp3nFFOSZbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. What are continuous and categorical variables?\n",
        "\n",
        ">- Continuous and categorical variables are two main types of variables you can work with in data and statistics.\n",
        "\n",
        "    Continuous variables:\n",
        "\n",
        ">- What they represent: Quantitative (numerical) data — they can take any value within a range.\n",
        "\n",
        ">- Examples: Height (169.5 cm), weight (72.3 kg), temperature (23°C), or time (5.7 seconds).\n",
        "\n",
        ">- Key feature: Between any two values, there’s an infinite number of possible values.\n",
        "\n",
        "    Categorical variables:\n",
        "\n",
        ">- What they represent: Qualitative (descriptive) data — they represent groups or categories.\n",
        "\n",
        ">- Examples: Gender (male/female), color (red/blue/green), grade (A, B, C), or country (Italy, Spain, USA).\n",
        "\n",
        ">- Key feature: Categories are discrete and separate; there’s no numerical range in which you can have fractional or intermediary values."
      ],
      "metadata": {
        "id": "U26ZV2d-S5n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        ">- Feature scaling refers to the process of standardizing or normalizing the range of independent variables or features in your dataset. This typically involves transforming the values to a common scale (like 0–1 or with a mean of 0 and standard deviation of 1).\n",
        "\n",
        "# How does it help in Machine Learning?\n",
        "\n",
        ">- Feature scaling is crucial for many algorithms due to their mathematical mechanisms:\n",
        "\n",
        ">- Distance-Based Methods (like KNN, K-Means): Without scaling, a feature with large values dominates the distance metric.\n",
        "\n",
        ">- Gradient Descent-Based Methods (like Logistic Regression, Neural Networks): Proper scaling helps the algorithm convergence faster and more stably.\n",
        "\n",
        ">- Regularization (like Ridge, Lasso): If features are not on a similar scale, penalties may be unfairly distributed across coefficients."
      ],
      "metadata": {
        "id": "RWOWPUcXTLIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. How do we perform scaling in Python?\n",
        "\n",
        ">- StandardScaler — Standardize (mean = 0, standard deviation = 1).\n",
        "\n",
        ">- MinMaxScaler — Normalize to range [0, 1].\n",
        "\n",
        ">- RobustScaler — Reduce influence of outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "g_dRn4KHT0y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. What is sklearn.preprocessing?\n",
        "\n",
        ">- sklearn.preprocessing refers to a module in scikit-learn (sklearn) that provides utilities for transforming raw data into a format that's more suitable for machine learning models.\n",
        "\n",
        "This typically involves scaling, normalizing, or encoding the data."
      ],
      "metadata": {
        "id": "HGlG4C0lUDu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        ">- To split your data into training and testing sets in Python, you typically use train_test_split from sklearn.model_selection.\n",
        "\n",
        "Here’s an example:\n",
        "\n",
        "    # First, import the function\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Say you have your data in X (features) and y (labels)\n",
        "    # Split into 80% training and 20% testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.20,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Now you can proceed with training:\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # And later evaluate:\n",
        "    model.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "DKHl12PxUTyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Explain data encoding?\n",
        "\n",
        ">- Data encoding refers to the process of converting information from one format or representation into another — typically for storage, transmission, or processing."
      ],
      "metadata": {
        "id": "1ugw0PtgUssP"
      }
    }
  ]
}